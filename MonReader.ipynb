{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7477442d-1907-4b90-a2e7-14547193d99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1e0ae0fc-e5a5-459e-875e-86b2c125667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path(r\"C:\\Users\\Charm\\Downloads\\images\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "076ab138-c856-4e98-a9c8-1526a5b8efe6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('C:/Users/Charm/Downloads/images/testing'),\n",
       " WindowsPath('C:/Users/Charm/Downloads/images/training')]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we need to confirm data presence, even though we know it exists\n",
    "\n",
    "list(DATA_ROOT.iterdir())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ba531df6-04fd-4a52-9b8f-dd3da55a07a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('C:/Users/Charm/Downloads/images/training/flip'),\n",
       " WindowsPath('C:/Users/Charm/Downloads/images/training/notflip')]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we need to comfirm whats containing inside training\n",
    "#the folders define the labels that will be needed \n",
    "list((DATA_ROOT / \"training\").iterdir())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "77049fb9-85eb-4b21-a236-9c251595a85c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1162"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we RESTRICT TO .JPG files because we verified the dataset during inspection\n",
    "#so we are going to go through them and count how many are there \n",
    "len(list((DATA_ROOT / \"training\" / \"flip\").glob(\"*.jpg\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6f0b3031-8277-4c04-89a1-5441b85915fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1230"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the next count should be for not flip\n",
    "len(list((DATA_ROOT /\"training\" / \"notflip\").glob(\"*.jpg\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1479ffcc-ee76-4353-81cd-09ab348e91f3",
   "metadata": {},
   "source": [
    "## Sofar we have training counts :\n",
    " 1. flip: 1162\n",
    " 2. notflip: 1230\n",
    "    This is almost 50/50\n",
    "## This is close to balanced, and it means :\n",
    " 1. We likely don't need class weights... this helps in punishing mistakes on rare classes heavily \n",
    " 2. F1 balances between precision: \"when the model saysflip, is it right? and Recall: does it catch most flips? so won't really be distorted by extreme imbalance... because classes are similar in size\n",
    " 3. A basic baseline model(the simplest reasonable model) is valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9782a983-8c4d-4499-90de-82ae1c1d24f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are going to store every .jpg filepath in the flip and store it\n",
    "#we are going to use this in our metadata table \n",
    "#(train_flip_path) is a list of path objects... where eachpath points to one \n",
    "#image file in the training/flip directory\n",
    "\n",
    "train_flip_paths = list((DATA_ROOT / \"training\" / \"flip\").glob(\"*.jpg\"))\n",
    "\n",
    "#WE are gonna do the same thing for the other side, which is not flip \n",
    "#this is important because we need both classes to build a metadata . _ \n",
    "train_notflip_paths = (list((DATA_ROOT /\"testing\" / \"flip\").glob(\"*.jpg\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ac72a442-2a79-486f-bcbd-7799d6606bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#next, we need to build a metadata table where each image path is paired with a label\n",
    "# metadata should contain the full filepath and a numeric label\n",
    "#because models operate on data loaded from a path and expect it in numeric form for loss(function) computation\n",
    "\n",
    "flip_rows = [(p, 1) for p in train_flip_paths] # now each flip is labelled.. contains (path, label)\n",
    "\n",
    "notflip_rows = [(p, 0) for p in train_notflip_paths]\n",
    " # now each notflip is labelled.. contains (path, label)\n",
    "\n",
    "# We need to mix these up so that training doesn't see all flips first and all not-flips.\n",
    "#we don't want bias .. we don't train on class A and then class B, we train on mixed batches\n",
    "train_rows = flip_rows + notflip_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d7df18aa-efd2-4675-852a-407c6d61ed0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#at this point, our data is no longer images, its a metadata about images\n",
    "#we have (path, label), this is tabular information\n",
    "#we switch to pandas dataframe, to add new columns, groupby, split train vs validation by video\n",
    "#one row will represent one image frame, identified by its filepath, with its corresponding class label\n",
    "\n",
    "df = pd.DataFrame(train_rows, columns=[\"filepath\", \"label\"]) #it adds colums(filename, label)\n",
    "\n",
    "#we then extract video_id from file name ... because file path has folders and slashes, and splitting, this can cause a mix \n",
    "#for every row we extract just the (image) filename and store it in filename\n",
    "df[\"filename\"] = df[\"filepath\"].apply(lambda p: Path(p).name)\n",
    "\n",
    "#now we create one for video, we taking the file, split on _ and keep the first part as video \n",
    "#if we split  by frames it create data leakage.. same frames can be in train, val or test \n",
    "# we want all frames from the same videos to stay in the same split (VideoID_FrameNumber.jpg)\n",
    "\n",
    "df[\"video_id\"] = df[\"filename\"].apply(lambda x: x.split(\"_\")[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a31ba605-6466-498f-a5df-0a644af84828",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"video_id\"].nunique() #how many unique videos exist in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a4724e3c-f8db-4cfb-ac9c-6532bf4e2284",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "322"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we need to shuffle videos first before setting validation, to avoid bias, and make sure validation is random\n",
    "#bias would be: videos recorded same day, same lighting, same document type, etc\n",
    "\n",
    "video_ids = df[\"video_id\"].unique() #we store all videos in numpy array\n",
    "rng = np.random.default_rng(42) #we need a fixed seed so that sampling is the same all times\n",
    "val_video_ids = rng.choice(video_ids, size=13, replace=False) #we randomly select 13 unique vidoes for validation\n",
    "#we create a boolean mask for validation rows\n",
    "df[\"is_val\"] = df[\"video_id\"].isin(val_video_ids) #is_val will assign true or false.. shows which vid are for val and train\n",
    "val_frames = df[df[\"is_val\"]].shape[0] #gets validation frame count\n",
    "val_frames #gives number of frame within validation\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5b038bb6-0417-464c-a988-8943750b33e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    255\n",
       "0     67\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to check class balance we will groupby label\n",
    "df[df[\"is_val\"]][\"label\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2214d36b-a0d8-48ec-a856-30b40cbc552e",
   "metadata": {},
   "source": [
    "## What we are aiming for: \n",
    "1. the model takes a single image frame as input and outputs a prediction of whether the page is flipping or not\n",
    "2.  A small CNN is the simplest model that can capture spatial patterns in images.\n",
    "3.  it exists to validate the data and task, not maximize performance\n",
    "So the model will learn by making predictions, measuring how wrong they are, and gradually adjusting itself to make fewer mistakes.\n",
    "## Since we are focused on bent page edges, curved lines, diagonal boundaries, and local distortion. \n",
    "* using small filters, let the network learn simple visual patterns first and combine them across layers to recognize larger structures, like flipping a page...\n",
    "* The idea of pooling will be useful to help the network generalize what matters(eg, what side of the page was bent happen... pooling (it doesn't matter) was the page bent?)\n",
    "F1 measures how good our flip/not-flip decisions are across videos. How often did we correctly predict\n",
    "## FINAL BIG IDEA :\n",
    "1. Convolution:\n",
    "   What visual cues exist --------> edges, bends, motion artifacts\n",
    "2. pooling\n",
    "   Do these cues exist anywhere -------> rebust to location\n",
    "3. Fully Connected------> Given all the cues, is it a flip?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9b18655b-2c63-40b0-aef0-4dfd59ec2592",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 1130 Val samples: 322\n",
      "images: torch.Size([32, 3, 224, 224])\n",
      "labels: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.utils.data import Dataset, DataLoader   #LETS you feed images to models in small batches\n",
    "from torchvision import transforms   # provided standard image processing/augmentation functions\n",
    "from PIL import Image   #reads/open image from disk into memory\n",
    "from pathlib import Path   #cleaner way to work with file paths\n",
    "import torch   #main engine that runs the neural network math (tensors, GPU/CPU ops, training, etc)\n",
    "\n",
    "# DATASET CLASS (the menu for my data ) you don't give everything at once, one at a time\n",
    "class FrameDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df.reset_index(drop=True)   # avoids index issues after filtering so every row has a simple index\n",
    "        self.transform = transform               #saves the rules for images, which should be modified(resize/rotate..)\n",
    "\n",
    "    #how many items do you have .. number of rows\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    #get me item say #57\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]               #go to the row number x in my table\n",
    "        filepath = row[\"filepath\"]            # column name must be \"filepath\"\n",
    "        label = row[\"label\"]                  # column name must be \"label\"\n",
    "\n",
    "        image = Image.open(filepath).convert(\"RGB\")     #open the image from disk and force it to be a normal colour \n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)            #if we have transform rules, apply them to the image \n",
    "\n",
    "        # BCEWithLogitsLoss expects float labels (0.0/1.0) form modeal to understand\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "#TRANSFORMS ... chains of them in order\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=10),               #slightly rotate the image so the model  dont panic\n",
    "    transforms.Resize((224, 224)),                       #make every image the exact same size so the model sees consistent input\n",
    "    transforms.ToTensor(),                               #convert images into numbers like[3,224, 224]\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])                                                     #these are ImageNet normalization stats , important cause resnets was trainined on pretrained imagenet\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),                    #for val we don't do random changes. We test the model on a stable version of the image\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "# break DATAFRAME INTO TRAIN / VAL\n",
    "train_df = df[~df[\"is_val\"]].reset_index(drop=True)  #one the model learns from\n",
    "val_df   = df[df[\"is_val\"]].reset_index(drop=True)   #checks if it truly learns \n",
    "\n",
    "print(\"Train samples:\", len(train_df), \"Val samples:\", len(val_df))  #quick count to confirm we actually have data in both groups\n",
    "\n",
    "\n",
    "#  DATASETS objects attach transform: its likecreating two menus\n",
    "# training has augmentation, validation does not \n",
    "train_dataset = FrameDataset(train_df, transform=train_transform)\n",
    "val_dataset   = FrameDataset(val_df, transform=val_transform)\n",
    "\n",
    "\n",
    "#DATALOADERS.. training takes 32 images at a time, and shuffle so the models don't learn in a predictable order\n",
    "# validation checks data in stable order-no shuffling needed (num_workers=0) avoids windows multipprocssing crushes\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,  num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "#Now let's pull one batch to confirm everything works before training\n",
    "images, labels = next(iter(train_loader))\n",
    "print(\"images:\", images.shape)   # expected: (B, 3, 224, 224)\n",
    "print(\"labels:\", labels.shape)   # expected: (B,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "096d1e63-6fa0-4125-a2df-d4e11b0939d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n",
      "logits shape: torch.Size([32, 1])\n",
      "labels shape: torch.Size([32, 1])\n",
      "loss: 0.836327314376831\n"
     ]
    }
   ],
   "source": [
    "#MODEL GIVES PRETRAINED cnns(LIKE RESNET)... .nn contains neural network layers and loses \n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "\n",
    "# 1) if we have GPU(makes training faster ) , we can use it otherwise use cpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "# 2) im staring with a model that already learned useful patterns from millions of images\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "in_features = model.fc.in_features                 #checks how many features the model produces before the final decision\n",
    "model.fc = nn.Linear(in_features, 1)   # I'm replacing the original 100-class with a simple yes/no decision head\n",
    "model = model.to(device)               #moves the model into the choses machine(cpu/gpu)\n",
    "\n",
    "# 3) loss function... the judge that punishes wrong yes/no predictions\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# 4) optimizer .. makes small improvements after each batch\n",
    "#lr=1e-4 is a safe starting learning rate \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)        #Adam updates parameters using gradients. lr=1e-4 is a cautious learning rate.\n",
    "\n",
    "# 5) one-batch forward pass sanity check (NO training yet)\n",
    "model.train()                                               #here model is intraining model\n",
    "images, labels = next(iter(train_loader))                    #grab one batch of images(tensr + labels) and answers\n",
    "images = images.to(device)\n",
    "labels = labels.to(device).float().unsqueeze(1)   # shape (B,1(reshape to ,atch model output)) to match logits( we move images and labels to same device as model)\n",
    "\n",
    "logits = model(images)                   # produce one raw score per image\n",
    "loss = criterion(logits, labels)         #BCE(the judge) compares model score to the trut and produce a penatlty number\n",
    "\n",
    "print(\"logits shape:\", logits.shape)\n",
    "print(\"labels shape:\", labels.shape)\n",
    "print(\"loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc151bea-9c9f-4483-b0cf-4f893c2a9ac9",
   "metadata": {},
   "source": [
    "## WHAT BCEWithLogitsLoss Actually Does\n",
    "So it's like two steps combined :\n",
    "## 1.  Turns the model's raw score into a probability\n",
    "   * Big positive number --> close to 1( very confdent flip)\n",
    "   * Big negative number --> close to 0 ( very confident not-flip)\n",
    "   * Near 0 --> i'm unsure\n",
    "   * These steps are what sigmoid normally does\n",
    "## 2.  Compares that probability to the real answer\n",
    "   * If true labels is 1(flip):\n",
    "      * High prob --> smal penalty\n",
    "      * Low prob --> big penanlty\n",
    "   * if the true label is 0(not flip):\n",
    "      * Low prob --> small penalty\n",
    "      * High prob --> big penalty\n",
    "## Loss does both steps together more safely than doing them separately  \n",
    "* The loss is a score that tells us how wrong the model is-- a high loss means its guessing, a lower means its learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a27bd075-ecc5-4981-be18-bd4a129bf977",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#this cell will teach the model once, then check whether it actually learned.\n",
    "# We will runone epoch... Show every training image to the model once, in small batches\n",
    "# after we test the model on validation images, it has never learned before\n",
    "\n",
    "import torch\n",
    "from torchmetrics.classification import BinaryF1Score # a fair grading rubric for yes/no questions\n",
    "\n",
    "# --- Metric helper --- this defines how we judge answers .. anything above fifty counts as yes\n",
    "f1_metric = BinaryF1Score(threshold=0.5).to(device)\n",
    "\n",
    "\n",
    "#an epoch = one full pass through all trainingimages, like studying one entire book once\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()               #training is on\n",
    "    running_loss = 0.0          \n",
    "\n",
    "    f1_metric.reset()            #we start fresh for the training session\n",
    "\n",
    "    #we study 32 batches at once instead of everything\n",
    "    for images, labels in loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)  # (B,1) e move everything to the same cpu and align the answer so they match format prediction\n",
    "\n",
    "        # 1) forward\n",
    "        logits = model(images)                  # (B,1) we look and say I think it's a flip, like raw opinions, not answers\n",
    "        loss = criterion(logits, labels)        # this creates a penalty score after compare answers\n",
    "\n",
    "        # 2) backward + update i.e learning from mistakes \n",
    "        optimizer.zero_grad()  # errase old result from training\n",
    "        loss.backward()        #figure out what went wrong\n",
    "        optimizer.step()        #adjust understanding slightly\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)   # we add up how bad the mistakes were, for all images\n",
    "                                                       # I think big batches would count more cuz they have more images\n",
    "\n",
    "        # 3) F1 needs predictions (0/1). logits -> prob -> pred\n",
    "        probs = torch.sigmoid(logits)           # (B,1) values in [0,1]\n",
    "        preds = (probs >= 0.5).float()          # (B,1) 0 or 1\n",
    "        f1_metric.update(preds, labels)         # update the report, like did we catch flips?\n",
    "\n",
    "    avg_loss = running_loss / len(loader.dataset)  #average mistakes level accross all study questions\n",
    "    avg_f1 = f1_metric.compute().item()            #overall F1 score for the session\n",
    "    return avg_loss, avg_f1\n",
    "\n",
    "#everything in validation , looks the same as training\n",
    "#just no backprogration, no optimizer and no learning , cant learn during exam\n",
    "@torch.no_grad() #this is exam mode now\n",
    "def validate_one_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    f1_metric.reset()\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)  # (B,1)\n",
    "\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs >= 0.5).float()\n",
    "        f1_metric.update(preds, labels)\n",
    "\n",
    "    avg_loss = running_loss / len(loader.dataset)\n",
    "    avg_f1 = f1_metric.compute().item()\n",
    "    return avg_loss, avg_f1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4adf1c04-ce32-472f-b6ff-e7c3f776ed8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | Train Loss: 0.6251  Train F1: 0.7870 | Val Loss: 0.5231  Val F1: 0.8839\n",
      "Epoch 2/5 | Train Loss: 0.4940  Train F1: 0.8871 | Val Loss: 0.5155  Val F1: 0.8839\n",
      "Epoch 3/5 | Train Loss: 0.4720  Train F1: 0.8863 | Val Loss: 0.5663  Val F1: 0.8839\n",
      "Epoch 4/5 | Train Loss: 0.4478  Train F1: 0.8866 | Val Loss: 0.5628  Val F1: 0.8761\n",
      "Epoch 5/5 | Train Loss: 0.4178  Train F1: 0.8919 | Val Loss: 0.5958  Val F1: 0.8246\n"
     ]
    }
   ],
   "source": [
    "# We're going to run the training\n",
    "num_epochs = 5    #how many val test we taking \n",
    "\n",
    "\n",
    "#each loop train, validate and report results\n",
    "# e.g., like how did u do while studying / and how did u do on the exam \n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss, train_f1 = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_f1     = validate_one_epoch(model, val_loader, criterion, device)\n",
    "\n",
    "    #we announce results on the screenboard\n",
    "    print(\n",
    "        f\"Epoch {epoch}/{num_epochs} | \"\n",
    "        f\"Train Loss: {train_loss:.4f}  Train F1: {train_f1:.4f} | \"\n",
    "        f\"Val Loss: {val_loss:.4f}  Val F1: {val_f1:.4f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30ed498-9258-42c9-9f82-611ccd4ddb15",
   "metadata": {},
   "source": [
    "## The model learned quickly, reached a strong performance, and generalized well to new images.\n",
    "## Training longer did not improve real-world performance, so the best model was selected early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ffbb7d-4056-4800-aecc-67adf46fb902",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
